---
output:
  pdf_document: default
  html_document: default
---

```{r}
library(ISLR)
ourAuto=data.frame("mpg"=Auto$mpg,"cylinders"=factor(cut(Auto$cylinders,2)),
                   "displace"=Auto$displacement,"horsepower"=Auto$horsepower,
                   "weight"=Auto$weight,"acceleration"=Auto$acceleration, 
                   "year"=Auto$year,"origin"=as.factor(Auto$origin))
colnames(ourAuto)
ntot=dim(ourAuto)[1]
ntot
set.seed(4268)
testids=sort(sample(1:ntot,ceiling(0.2*ntot),replace=FALSE))
ourAutoTrain=ourAuto[-testids,]
ourAutoTest=ourAuto[testids,]
```
# Problem 2 - Shrinkage methods 

## a) Lasso and ridge regression

* Q11: $\lambda$ is a parameter which draws the estimated coeffiecients towards zero in both ridge and lasso regression. For very large $\lambda$'s, all the estimated coefficients are essentially zero. However, in ridge regression, none of the coefficients will be set exact to zero. In comparison , the penalty term in lasso regression may set some of the coefficient to be exactly zero when $\lambda$ is large enough. In Figure 1, we can see that this is the case, while the coefficients in Figure 2 do not seem to be exactly zero for any value of $\lambda$ in the given interval. Thus, Figure 1 corresponds to lasso regression while Figure 2 corresponds to ridge regression.

* Q12: As stated, we can see that the tuning parameter $\lambda$ draws the $\beta$'s towards zero as $\lambda$ increases. In lasso regression, all of the coefficients seem to become exactly zero, while in ridge regression, the coefficients are drawn towards zero, but are not exactly zero. When $\lambda=0$, the penalty term in both ridge and lasso regression will disappear, so the coefficients will just be the same as with least squares estimation. At this value for $\lambda$, the bias is zero, but the variance may be high. As $\lambda$ increases, the variance will become lower as the coefficients are shrinked and flexibility decreases. However, the bias will increase slightly at the same time. In both ridge and lasso regression, when $\lambda \rightarrow \infty$, we have the null model, which means that all of the coefficient estimates are zero. Then the variance approaches zero, but the bias becomes large.

* Q13: Ridge regression will include all $p$ covariates, and thus can not be used to perform model selection. However, lasso regression can be used in the same way as best subset selection, as the variables are forced to be exactly zero when $\lambda$ is large. For a given value of $\lambda$, lasso regression may zero out some of the estimated coefficients and it thus performs a variable selection.  

##b) Finding the optimal $\lambda$
* Q14: The function cv.glmnet performs a k-fold cross-validation:

```{r}
library(glmnet)
set.seed(4268)

x=model.matrix(mpg~.,ourAutoTrain)[,-1] #-1 to remove the intercept.
head(x)
y=ourAutoTrain$mpg

lambda=c(seq(from=5,to=0.1,length.out=150),0.01,0.0001) #Create a set of tuning parameters, adding low value to also see least squares fit
cv.out=cv.glmnet(x,y,alpha=1,nfolds=10,lambda=lambda, standardize=TRUE) #alpha=1 gives lasso, alpha=0 gives ridge

plot(cv.out)
```


* Q15: The plot shows the cross-validation curve, including the upper and lower standard deviation curves along the sequence of $\lambda$'s. The $\lambda$ with the lowest cross-validated MSE in the plot can be chosen as the optimal $\lambda$:
```{r}
cv.out$lambda.min
```

* Q16: The `1se-rule`, is another way to choose which $\lambda$ is optimal. In the object returned by `cv.glmnet`, one of the values is `lambda.1se`, which is the largest value of $\lambda$ such that the error is within one standard error of the minimum. Here, we get that this lambda is given by
```{r}
cv.out$lambda.1se
```


##c)
* Q17: Using lasso regression with the optimal value of $\lambda$ according to the `1se-rule`, $\lambda=0.01$, we can fit the model. The coefficient estimates are given by 
```{r}
coef(cv.out,s="lambda.1se")
```

* Q18: Using this model with optimal $\lambda$ chosen according to the `1se-rule`, we can predict `mpg` for cars based on their data. For a car with 4 cylinders, `displace=150`, `horsepower=100`, `weight=3000`, `acceleration=10`, `year=82` and which comes from Europe, `mpg` is predicted as
```{r}
predict(cv.out, newx =matrix(c(0,150,100,3000,10,82,1,0),nrow=1), s = "lambda.1se")
```

