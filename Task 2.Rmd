# Problem 2 - Shrinkage methods 

## a) Lasso and ridge regression

* Q11: $\lambda$ is a parameter which draws the estimated coeffiecients towards zero in both ridge and lasso regression. For very large $\lambda$'s, all the estimated coefficients are essentially zero. However, in ridge regression, none of the coefficients will be set exact to zero. In comparison , the penalty term in lasso regression may set some of the coefficient to be exactly zero when $\lambda$ is large enough. In Figure 1, we can see that this is the case, while the coefficients in Figure 2 do not seem to be exactly zero for any value of $\lambda$ in the given interval. Thus, Figure 1 corresponds to lasso regression while Figure 2 corresponds to ridge regression.

* Q12: As stated, we can see that the tuning parameter $\lambda$ draws the $\beta$'s towards zero as $\lambda$ increases. In lasso regression, all of the coefficients seem to become exactly zero, while in ridge regression, the coefficients are drawn towards zero, but are not exactly zero. When $\lambda=0$, the penalty term in both ridge and lasso regression will disappear, so the coefficients will just be the same as with least squares estimation. At this value for $\lambda$, the bias is zero, but the variance may be high. As $\lambda$ increases, the variance will become lower as the coefficients are shrinked and flexibility decreases. However, the bias will increase slightly at the same time. In both ridge and lasso regression, when $\lambda \rightarrow \infty$, we have the null model, which means that all of the coefficient estimates are zero. Then the variance approaches zero, but the bias becomes large.

* Q13: 

##b) Finding the optimal $\lambda$
* Q14: The function cv.glmnet performs a k-fold cross-validation 

